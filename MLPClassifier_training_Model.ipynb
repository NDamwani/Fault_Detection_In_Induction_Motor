{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3146af93",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pgmpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StandardScaler, LabelEncoder\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpgmpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BayesianModel\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpgmpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfactors\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdiscrete\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TabularCPD\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpgmpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minference\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VariableElimination\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pgmpy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from pgmpy.models import BayesianModel\n",
    "from pgmpy.factors.discrete import TabularCPD\n",
    "from pgmpy.inference import VariableElimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72534d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreprocessor:\n",
    "    def __init__(self, filepath):\n",
    "        self.filepath = filepath\n",
    "        self.label_encoder = LabelEncoder()\n",
    "\n",
    "    def load_data(self):\n",
    "        self.data = pd.read_excel(self.filepath, header=0)\n",
    "        self.features = self.data.iloc[:, :-1]\n",
    "        self.labels = self.data.iloc[:, -1]\n",
    "\n",
    "    def clean_data(self):\n",
    "        self.features = self.features.apply(pd.to_numeric, errors='coerce')\n",
    "        self.features = self.features.fillna(self.features.mean())\n",
    "\n",
    "    def encode_labels(self):\n",
    "        self.labels = pd.to_numeric(self.labels, errors='coerce')\n",
    "        valid_indices = self.labels.dropna().index\n",
    "        self.labels = self.labels.loc[valid_indices].astype(int)\n",
    "        self.features = self.features.loc[valid_indices]\n",
    "        self.labels = self.label_encoder.fit_transform(self.labels)\n",
    "\n",
    "    def scale_features(self):\n",
    "        scaler = StandardScaler()\n",
    "        self.features = scaler.fit_transform(self.features)\n",
    "\n",
    "    def split_data(self):\n",
    "        return train_test_split(self.features, self.labels, test_size=0.2, random_state=42)\n",
    "\n",
    "    def preprocess(self):\n",
    "        self.load_data()\n",
    "        self.clean_data()\n",
    "        self.encode_labels()\n",
    "        self.scale_features()\n",
    "        return self.split_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbb48915",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPFaultModel:\n",
    "    def __init__(self, num_inputs):\n",
    "        self.model = MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=1500, alpha=0.001, random_state=1, learning_rate_init=0.001)\n",
    "\n",
    "    def train(self, X_train, y_train):\n",
    "        self.model.fit(X_train, y_train)\n",
    "\n",
    "    def evaluate(self, X_test, y_test):\n",
    "        predictions = self.model.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, predictions)\n",
    "        report = classification_report(y_test, predictions)\n",
    "        return accuracy, report\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return self.model.predict_proba(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11b14c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPFaultModelManager:\n",
    "    def __init__(self, input_dim, num_conditions):\n",
    "        self.models = {condition: MLPFaultModel(input_dim) for condition in range(num_conditions)}\n",
    "\n",
    "    def train_models(self, X_train, y_train):\n",
    "        for condition, model in self.models.items():\n",
    "            binary_labels = (y_train == condition).astype(int)\n",
    "            model.train(X_train, binary_labels)\n",
    "\n",
    "    def evaluate_models(self, X_test, y_test):\n",
    "        evaluations = {}\n",
    "        for condition, model in self.models.items():\n",
    "            binary_labels = (y_test == condition).astype(int)\n",
    "            accuracy, report = model.evaluate(X_test, binary_labels)\n",
    "            evaluations[condition] = {'accuracy': accuracy, 'report': report}\n",
    "        return evaluations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b80933ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualCalculator:\n",
    "    def __init__(self, models):\n",
    "        self.models = models\n",
    "\n",
    "    def calculate_residuals(self, X, y):\n",
    "        residuals = {}\n",
    "        for condition, model in self.models.items():\n",
    "            probabilities = model.predict_proba(X)[:, 1]\n",
    "            actual = (y == condition).astype(int)\n",
    "            residuals[condition] = np.abs(probabilities - actual)\n",
    "        return residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c75b87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicBayesianNetwork:\n",
    "    def __init__(self, conditions):\n",
    "        self.conditions = conditions\n",
    "        self.model = BayesianModel()\n",
    "        self.setup_model()\n",
    "        self.inference = VariableElimination(self.model)\n",
    "\n",
    "    def setup_model(self):\n",
    "        condition_nodes = ['Condition_' + str(cond) for cond in self.conditions]\n",
    "        self.model.add_nodes_from(['State'] + condition_nodes)\n",
    "        self.model.add_edges_from([('State', node) for node in condition_nodes])\n",
    "        cpd_state = TabularCPD(variable='State', variable_card=2, values=[[0.5], [0.5]])\n",
    "        self.model.add_cpds(cpd_state)\n",
    "        for node in condition_nodes:\n",
    "            cpd = TabularCPD(variable=node, variable_card=2, values=[[0.95, 0.05], [0.05, 0.95]], evidence=['State'], evidence_card=[2])\n",
    "            self.model.add_cpds(cpd)\n",
    "        assert self.model.check_model(), \"Model configuration errors\"\n",
    "\n",
    "    def integrate_with_dbn(self, evidence):\n",
    "        return self.inference.query(variables=['State'], evidence=evidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc46f12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "class ModelEvaluator:\n",
    "    def __init__(self, models):\n",
    "        self.models = models\n",
    "\n",
    "    def evaluate_models(self, X_test, y_test):\n",
    "        evaluations = {}\n",
    "        for condition, model in self.models.items():\n",
    "            accuracy, report = model.evaluate(X_test, (y_test == condition).astype(int))\n",
    "            evaluations[condition] = {'accuracy': accuracy, 'report': report}\n",
    "        return evaluations\n",
    "\n",
    "    def predict_random_samples(self, X_test, y_test, n_samples=500):\n",
    "        indices = np.random.choice(len(X_test), size=n_samples, replace=False)\n",
    "        predictions = [np.argmax([model.predict_proba(X_test[idx:idx+1])[:, 1] for model in self.models.values()], axis=0) for idx in indices]\n",
    "        actuals = y_test[indices]\n",
    "        accuracy = accuracy_score(actuals, predictions)\n",
    "        # Create DataFrame for better visualization\n",
    "        results_df = pd.DataFrame({\n",
    "            'Sample Index': indices,\n",
    "            'Predicted Class': predictions,\n",
    "            'Actual Class': actuals\n",
    "        })\n",
    "        return results_df, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d80145c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pgmpy:BayesianModel has been renamed to BayesianNetwork. Please use BayesianNetwork class, BayesianModel will be removed in future.\n",
      "WARNING:pgmpy:BayesianModel has been renamed to BayesianNetwork. Please use BayesianNetwork class, BayesianModel will be removed in future.\n",
      "WARNING:pgmpy:BayesianModel has been renamed to BayesianNetwork. Please use BayesianNetwork class, BayesianModel will be removed in future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Model Evaluations:\n",
      "Condition 0: Accuracy = 100.00%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1493\n",
      "           1       1.00      1.00      1.00       108\n",
      "\n",
      "    accuracy                           1.00      1601\n",
      "   macro avg       1.00      1.00      1.00      1601\n",
      "weighted avg       1.00      1.00      1.00      1601\n",
      "\n",
      "Condition 1: Accuracy = 100.00%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1506\n",
      "           1       1.00      1.00      1.00        95\n",
      "\n",
      "    accuracy                           1.00      1601\n",
      "   macro avg       1.00      1.00      1.00      1601\n",
      "weighted avg       1.00      1.00      1.00      1601\n",
      "\n",
      "Condition 2: Accuracy = 100.00%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1487\n",
      "           1       1.00      1.00      1.00       114\n",
      "\n",
      "    accuracy                           1.00      1601\n",
      "   macro avg       1.00      1.00      1.00      1601\n",
      "weighted avg       1.00      1.00      1.00      1601\n",
      "\n",
      "Condition 3: Accuracy = 98.13%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99      1495\n",
      "           1       0.96      0.75      0.84       106\n",
      "\n",
      "    accuracy                           0.98      1601\n",
      "   macro avg       0.97      0.87      0.92      1601\n",
      "weighted avg       0.98      0.98      0.98      1601\n",
      "\n",
      "Condition 4: Accuracy = 100.00%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1505\n",
      "           1       1.00      1.00      1.00        96\n",
      "\n",
      "    accuracy                           1.00      1601\n",
      "   macro avg       1.00      1.00      1.00      1601\n",
      "weighted avg       1.00      1.00      1.00      1601\n",
      "\n",
      "Condition 5: Accuracy = 99.25%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00      1505\n",
      "           1       0.91      0.97      0.94        96\n",
      "\n",
      "    accuracy                           0.99      1601\n",
      "   macro avg       0.95      0.98      0.97      1601\n",
      "weighted avg       0.99      0.99      0.99      1601\n",
      "\n",
      "Condition 6: Accuracy = 99.94%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1489\n",
      "           1       1.00      0.99      1.00       112\n",
      "\n",
      "    accuracy                           1.00      1601\n",
      "   macro avg       1.00      1.00      1.00      1601\n",
      "weighted avg       1.00      1.00      1.00      1601\n",
      "\n",
      "Condition 7: Accuracy = 97.63%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99      1515\n",
      "           1       0.86      0.66      0.75        86\n",
      "\n",
      "    accuracy                           0.98      1601\n",
      "   macro avg       0.92      0.83      0.87      1601\n",
      "weighted avg       0.97      0.98      0.97      1601\n",
      "\n",
      "Condition 8: Accuracy = 98.94%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      1510\n",
      "           1       0.95      0.86      0.90        91\n",
      "\n",
      "    accuracy                           0.99      1601\n",
      "   macro avg       0.97      0.93      0.95      1601\n",
      "weighted avg       0.99      0.99      0.99      1601\n",
      "\n",
      "Condition 9: Accuracy = 99.06%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99      1492\n",
      "           1       0.91      0.95      0.93       109\n",
      "\n",
      "    accuracy                           0.99      1601\n",
      "   macro avg       0.95      0.97      0.96      1601\n",
      "weighted avg       0.99      0.99      0.99      1601\n",
      "\n",
      "Condition 10: Accuracy = 100.00%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1503\n",
      "           1       1.00      1.00      1.00        98\n",
      "\n",
      "    accuracy                           1.00      1601\n",
      "   macro avg       1.00      1.00      1.00      1601\n",
      "weighted avg       1.00      1.00      1.00      1601\n",
      "\n",
      "Condition 11: Accuracy = 100.00%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1491\n",
      "           1       1.00      1.00      1.00       110\n",
      "\n",
      "    accuracy                           1.00      1601\n",
      "   macro avg       1.00      1.00      1.00      1601\n",
      "weighted avg       1.00      1.00      1.00      1601\n",
      "\n",
      "Condition 12: Accuracy = 100.00%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1511\n",
      "           1       1.00      1.00      1.00        90\n",
      "\n",
      "    accuracy                           1.00      1601\n",
      "   macro avg       1.00      1.00      1.00      1601\n",
      "weighted avg       1.00      1.00      1.00      1601\n",
      "\n",
      "Condition 13: Accuracy = 100.00%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1498\n",
      "           1       1.00      1.00      1.00       103\n",
      "\n",
      "    accuracy                           1.00      1601\n",
      "   macro avg       1.00      1.00      1.00      1601\n",
      "weighted avg       1.00      1.00      1.00      1601\n",
      "\n",
      "Condition 14: Accuracy = 99.75%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1520\n",
      "           1       0.99      0.96      0.97        81\n",
      "\n",
      "    accuracy                           1.00      1601\n",
      "   macro avg       0.99      0.98      0.99      1601\n",
      "weighted avg       1.00      1.00      1.00      1601\n",
      "\n",
      "Condition 15: Accuracy = 100.00%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1495\n",
      "           1       1.00      1.00      1.00       106\n",
      "\n",
      "    accuracy                           1.00      1601\n",
      "   macro avg       1.00      1.00      1.00      1601\n",
      "weighted avg       1.00      1.00      1.00      1601\n",
      "\n",
      "Feedback from DBN: +----------+--------------+\n",
      "| State    |   phi(State) |\n",
      "+==========+==============+\n",
      "| State(0) |       1.0000 |\n",
      "+----------+--------------+\n",
      "| State(1) |       0.0000 |\n",
      "+----------+--------------+\n",
      "Final Model Evaluations after DBN Feedback:\n",
      "Condition 0: Accuracy = 100.00%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1493\n",
      "           1       1.00      1.00      1.00       108\n",
      "\n",
      "    accuracy                           1.00      1601\n",
      "   macro avg       1.00      1.00      1.00      1601\n",
      "weighted avg       1.00      1.00      1.00      1601\n",
      "\n",
      "Condition 1: Accuracy = 100.00%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1506\n",
      "           1       1.00      1.00      1.00        95\n",
      "\n",
      "    accuracy                           1.00      1601\n",
      "   macro avg       1.00      1.00      1.00      1601\n",
      "weighted avg       1.00      1.00      1.00      1601\n",
      "\n",
      "Condition 2: Accuracy = 100.00%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1487\n",
      "           1       1.00      1.00      1.00       114\n",
      "\n",
      "    accuracy                           1.00      1601\n",
      "   macro avg       1.00      1.00      1.00      1601\n",
      "weighted avg       1.00      1.00      1.00      1601\n",
      "\n",
      "Condition 3: Accuracy = 98.13%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99      1495\n",
      "           1       0.96      0.75      0.84       106\n",
      "\n",
      "    accuracy                           0.98      1601\n",
      "   macro avg       0.97      0.87      0.92      1601\n",
      "weighted avg       0.98      0.98      0.98      1601\n",
      "\n",
      "Condition 4: Accuracy = 100.00%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1505\n",
      "           1       1.00      1.00      1.00        96\n",
      "\n",
      "    accuracy                           1.00      1601\n",
      "   macro avg       1.00      1.00      1.00      1601\n",
      "weighted avg       1.00      1.00      1.00      1601\n",
      "\n",
      "Condition 5: Accuracy = 99.25%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00      1505\n",
      "           1       0.91      0.97      0.94        96\n",
      "\n",
      "    accuracy                           0.99      1601\n",
      "   macro avg       0.95      0.98      0.97      1601\n",
      "weighted avg       0.99      0.99      0.99      1601\n",
      "\n",
      "Condition 6: Accuracy = 99.94%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1489\n",
      "           1       1.00      0.99      1.00       112\n",
      "\n",
      "    accuracy                           1.00      1601\n",
      "   macro avg       1.00      1.00      1.00      1601\n",
      "weighted avg       1.00      1.00      1.00      1601\n",
      "\n",
      "Condition 7: Accuracy = 97.63%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99      1515\n",
      "           1       0.86      0.66      0.75        86\n",
      "\n",
      "    accuracy                           0.98      1601\n",
      "   macro avg       0.92      0.83      0.87      1601\n",
      "weighted avg       0.97      0.98      0.97      1601\n",
      "\n",
      "Condition 8: Accuracy = 98.94%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      1510\n",
      "           1       0.95      0.86      0.90        91\n",
      "\n",
      "    accuracy                           0.99      1601\n",
      "   macro avg       0.97      0.93      0.95      1601\n",
      "weighted avg       0.99      0.99      0.99      1601\n",
      "\n",
      "Condition 9: Accuracy = 99.06%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99      1492\n",
      "           1       0.91      0.95      0.93       109\n",
      "\n",
      "    accuracy                           0.99      1601\n",
      "   macro avg       0.95      0.97      0.96      1601\n",
      "weighted avg       0.99      0.99      0.99      1601\n",
      "\n",
      "Condition 10: Accuracy = 100.00%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1503\n",
      "           1       1.00      1.00      1.00        98\n",
      "\n",
      "    accuracy                           1.00      1601\n",
      "   macro avg       1.00      1.00      1.00      1601\n",
      "weighted avg       1.00      1.00      1.00      1601\n",
      "\n",
      "Condition 11: Accuracy = 100.00%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1491\n",
      "           1       1.00      1.00      1.00       110\n",
      "\n",
      "    accuracy                           1.00      1601\n",
      "   macro avg       1.00      1.00      1.00      1601\n",
      "weighted avg       1.00      1.00      1.00      1601\n",
      "\n",
      "Condition 12: Accuracy = 100.00%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1511\n",
      "           1       1.00      1.00      1.00        90\n",
      "\n",
      "    accuracy                           1.00      1601\n",
      "   macro avg       1.00      1.00      1.00      1601\n",
      "weighted avg       1.00      1.00      1.00      1601\n",
      "\n",
      "Condition 13: Accuracy = 100.00%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1498\n",
      "           1       1.00      1.00      1.00       103\n",
      "\n",
      "    accuracy                           1.00      1601\n",
      "   macro avg       1.00      1.00      1.00      1601\n",
      "weighted avg       1.00      1.00      1.00      1601\n",
      "\n",
      "Condition 14: Accuracy = 99.75%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1520\n",
      "           1       0.99      0.96      0.97        81\n",
      "\n",
      "    accuracy                           1.00      1601\n",
      "   macro avg       0.99      0.98      0.99      1601\n",
      "weighted avg       1.00      1.00      1.00      1601\n",
      "\n",
      "Condition 15: Accuracy = 100.00%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1495\n",
      "           1       1.00      1.00      1.00       106\n",
      "\n",
      "    accuracy                           1.00      1601\n",
      "   macro avg       1.00      1.00      1.00      1601\n",
      "weighted avg       1.00      1.00      1.00      1601\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random Sample Evaluations (500 samples):\n",
      " Sample Index Predicted Class  Actual Class\n",
      "          165             [9]             3\n",
      "          446             [7]             7\n",
      "          572             [1]             1\n",
      "          776            [13]            13\n",
      "         1330             [0]             0\n",
      "          196             [8]             8\n",
      "          239             [2]             2\n",
      "          787            [10]            10\n",
      "         1032             [4]             4\n",
      "          747            [13]            13\n",
      "          899             [4]             4\n",
      "          961             [9]             9\n",
      "         1007             [5]             5\n",
      "          616             [2]             2\n",
      "          148             [5]             5\n",
      "         1405             [8]             8\n",
      "          312             [4]             4\n",
      "          422            [13]            13\n",
      "          217            [10]            10\n",
      "          769            [11]            11\n",
      "         1345             [8]             8\n",
      "          710             [0]             0\n",
      "         1444            [14]            14\n",
      "         1213             [2]             2\n",
      "          943             [8]             8\n",
      "          103            [11]            11\n",
      "          159             [0]             0\n",
      "          101             [2]             2\n",
      "         1463             [6]             6\n",
      "         1225            [15]            15\n",
      "         1186            [13]            13\n",
      "           28             [1]             1\n",
      "          216             [4]             4\n",
      "           29            [13]            13\n",
      "          376             [2]             2\n",
      "         1384            [14]            14\n",
      "         1245            [11]            11\n",
      "          586             [6]             6\n",
      "          550            [12]            12\n",
      "          292             [2]             2\n",
      "          812             [3]             3\n",
      "         1250             [7]             7\n",
      "          665            [10]            10\n",
      "          989            [10]            10\n",
      "          834            [12]            12\n",
      "          427            [12]            12\n",
      "          865             [0]             0\n",
      "          334            [13]            13\n",
      "          732             [9]             3\n",
      "          117             [2]             2\n",
      "          723            [12]            12\n",
      "         1226            [13]            13\n",
      "          692            [15]            15\n",
      "          303            [14]            14\n",
      "         1068            [11]            11\n",
      "          527            [10]            10\n",
      "         1497             [2]             2\n",
      "          123             [9]            14\n",
      "          138            [15]            15\n",
      "          399             [9]             9\n",
      "         1181            [12]            12\n",
      "          600             [7]             7\n",
      "          592             [0]             0\n",
      "          471             [9]             9\n",
      "         1281            [12]            12\n",
      "          895             [4]             4\n",
      "          512             [9]             9\n",
      "          360            [15]            15\n",
      "          908             [0]             0\n",
      "          197             [5]             5\n",
      "          876             [4]             4\n",
      "          912            [15]            15\n",
      "          439            [10]            10\n",
      "          238            [12]            12\n",
      "         1020             [0]             0\n",
      "          580             [3]             3\n",
      "          435            [12]            12\n",
      "          728            [11]            11\n",
      "         1447             [8]             8\n",
      "          425             [7]             7\n",
      "          886             [9]             9\n",
      "          525            [11]            11\n",
      "         1284            [14]            14\n",
      "         1289             [6]             6\n",
      "          177             [9]             9\n",
      "         1188            [11]            11\n",
      "         1027            [14]            14\n",
      "          470            [12]            12\n",
      "          835             [4]             4\n",
      "          942             [2]             2\n",
      "         1122             [1]             1\n",
      "          956            [11]            11\n",
      "         1247             [4]             4\n",
      "         1356             [0]             0\n",
      "          687            [14]            14\n",
      "         1537             [3]             3\n",
      "          591            [15]            15\n",
      "            3            [12]            12\n",
      "          939             [9]             9\n",
      "          810             [2]             2\n",
      "          913            [14]            14\n",
      "          900             [1]             1\n",
      "         1549            [13]            13\n",
      "         1565             [2]             2\n",
      "          785            [11]            11\n",
      "          850             [6]             6\n",
      "          451             [3]             7\n",
      "         1086             [6]             6\n",
      "         1158             [2]             2\n",
      "          846             [0]             0\n",
      "          528             [5]             5\n",
      "         1514             [6]             6\n",
      "         1351             [0]             0\n",
      "          871             [3]             3\n",
      "          287             [7]             7\n",
      "         1008             [6]             6\n",
      "          321             [2]             2\n",
      "         1231             [2]             2\n",
      "          815             [1]             1\n",
      "          947             [4]             4\n",
      "          694             [6]             6\n",
      "          267            [14]            14\n",
      "          726             [4]             4\n",
      "         1476             [9]             9\n",
      "            5            [12]            12\n",
      "          280             [9]             9\n",
      "            9            [10]            10\n",
      "          604            [14]            14\n",
      "         1437             [5]             5\n",
      "          974             [7]             7\n",
      "         1116            [12]            12\n",
      "           70             [9]             9\n",
      "         1261            [12]            12\n",
      "          877             [6]             6\n",
      "          251            [14]            14\n",
      "           74             [3]             3\n",
      "          873             [1]             1\n",
      "         1424             [3]             3\n",
      "          420             [0]             0\n",
      "          490             [0]             0\n",
      "          630            [15]            15\n",
      "         1378            [11]            11\n",
      "          308             [0]             0\n",
      "          545             [5]             5\n",
      "          371             [5]             5\n",
      "         1229            [14]            14\n",
      "          964            [12]            12\n",
      "          693             [9]             9\n",
      "          152             [3]             3\n",
      "          426             [0]             0\n",
      "          644            [14]            14\n",
      "          715             [3]             3\n",
      "          948             [1]             1\n",
      "         1525             [7]             7\n",
      "         1056            [14]            14\n",
      "         1296             [2]             2\n",
      "          645            [13]            13\n",
      "         1114            [11]            11\n",
      "          729            [15]            15\n",
      "          213             [6]             6\n",
      "         1061             [1]             1\n",
      "          611             [8]             8\n",
      "          736             [2]             2\n",
      "          650             [7]             7\n",
      "          796             [9]             9\n",
      "          612             [0]             0\n",
      "          305            [15]            15\n",
      "         1199            [14]            14\n",
      "          830             [3]             3\n",
      "          960             [1]             1\n",
      "          686            [13]            13\n",
      "         1021            [11]            11\n",
      "         1462             [5]             5\n",
      "          270            [10]            10\n",
      "          577             [1]             1\n",
      "         1434            [12]            12\n",
      "         1360             [5]             5\n",
      "          571             [3]             3\n",
      "         1465             [5]             5\n",
      "          915             [1]             1\n",
      "          168             [4]             4\n",
      "         1581             [1]             1\n",
      "          623            [12]            12\n",
      "         1038             [4]             4\n",
      "          741            [15]            15\n",
      "         1092             [0]             0\n",
      "          318             [0]             0\n",
      "          377             [4]             4\n",
      "         1214             [3]             3\n",
      "          189             [2]             2\n",
      "          182             [1]             1\n",
      "          917             [8]             8\n",
      "         1567             [3]             3\n",
      "         1018            [11]            11\n",
      "          296            [10]            10\n",
      "         1216             [6]             6\n",
      "          129             [3]             3\n",
      "         1080             [5]             5\n",
      "           66            [12]            12\n",
      "          398             [9]             9\n",
      "          508             [9]             9\n",
      "         1143             [1]             1\n",
      "          678             [2]             2\n",
      "         1095             [4]             4\n",
      "           83            [15]            15\n",
      "         1176            [12]            12\n",
      "          277             [0]             0\n",
      "          445            [13]            13\n",
      "           48             [7]             9\n",
      "           36             [4]             4\n",
      "          264             [4]             4\n",
      "          808            [13]            13\n",
      "          761            [12]            12\n",
      "         1544            [15]            15\n",
      "         1415             [2]             2\n",
      "         1547             [8]             8\n",
      "          829            [15]            15\n",
      "         1443             [1]             1\n",
      "          476             [6]             6\n",
      "         1198             [7]             7\n",
      "         1575            [14]            14\n",
      "          907             [1]             1\n",
      "          959             [5]             5\n",
      "          378             [7]             7\n",
      "          566             [2]             2\n",
      "         1366             [5]             5\n",
      "          643             [8]             8\n",
      "          224            [14]            14\n",
      "         1541             [8]             8\n",
      "         1127             [9]             9\n",
      "         1121             [9]             9\n",
      "         1052             [9]             7\n",
      "          918             [8]             8\n",
      "           67            [10]            10\n",
      "           51             [8]             8\n",
      "          621            [10]            10\n",
      "         1088             [2]             2\n",
      "          142            [13]            13\n",
      "         1470            [11]            11\n",
      "           71             [0]             0\n",
      "         1191             [8]             8\n",
      "         1336             [9]             9\n",
      "          784            [12]            12\n",
      "          970            [13]            13\n",
      "         1137             [0]             0\n",
      "          337            [10]            10\n",
      "          658            [10]            10\n",
      "          655             [1]             1\n",
      "         1260             [8]             8\n",
      "         1039             [0]             0\n",
      "          928             [5]             8\n",
      "         1107             [3]             3\n",
      "          905            [11]            11\n",
      "         1026             [5]             5\n",
      "            7             [2]             2\n",
      "         1152             [6]             6\n",
      "         1515             [0]             0\n",
      "          465            [14]            14\n",
      "          872            [10]            10\n",
      "         1302             [8]             8\n",
      "         1471             [5]             5\n",
      "         1580             [9]             9\n",
      "         1096            [15]            15\n",
      "          202             [7]             7\n",
      "          114             [3]             3\n",
      "          718            [10]            10\n",
      "         1494            [13]            13\n",
      "         1425             [2]             2\n",
      "          555            [12]            12\n",
      "          614             [8]             8\n",
      "          773             [6]             6\n",
      "         1499            [11]            11\n",
      "          122             [9]             9\n",
      "         1483             [4]             4\n",
      "          669             [5]             5\n",
      "          194            [14]            14\n",
      "          783            [14]            14\n",
      "          260             [4]             4\n",
      "          744             [0]             0\n",
      "         1316             [7]             7\n",
      "          489             [4]             4\n",
      "         1576             [5]             5\n",
      "         1102            [11]            11\n",
      "          825             [6]             6\n",
      "         1147             [5]             5\n",
      "          407             [6]             6\n",
      "         1448            [11]            11\n",
      "          269            [12]            12\n",
      "          444             [8]             8\n",
      "         1338             [2]             2\n",
      "         1592             [9]             9\n",
      "          598             [5]             5\n",
      "          307            [10]            10\n",
      "         1551            [13]            13\n",
      "          440            [10]            10\n",
      "          659            [15]            15\n",
      "         1201             [3]             3\n",
      "          620             [9]             9\n",
      "         1217            [15]            15\n",
      "          468            [12]            12\n",
      "         1395             [9]             9\n",
      "         1207            [12]            12\n",
      "         1136            [14]            14\n",
      "          882             [3]             3\n",
      "         1140             [0]             0\n",
      "          569            [11]            11\n",
      "         1305            [13]            13\n",
      "         1263            [15]            15\n",
      "          637            [11]            11\n",
      "          627            [14]            14\n",
      "         1398             [9]             9\n",
      "         1079            [11]            11\n",
      "         1060             [3]             3\n",
      "          985             [4]             4\n",
      "          557            [11]            11\n",
      "         1505            [10]            10\n",
      "         1228             [6]             6\n",
      "          143            [15]            15\n",
      "          434             [1]             1\n",
      "          201             [3]             3\n",
      "         1418             [1]             1\n",
      "          266            [12]            12\n",
      "          717             [3]             3\n",
      "         1046             [7]             7\n",
      "         1401            [13]            13\n",
      "         1532            [10]            10\n",
      "          828            [11]            11\n",
      "         1109            [14]            14\n",
      "          167             [9]             9\n",
      "         1431            [13]            13\n",
      "          133             [7]             7\n",
      "          901            [15]            15\n",
      "          791             [0]             0\n",
      "         1183            [15]            15\n",
      "         1417             [0]             0\n",
      "          198             [4]             4\n",
      "         1328             [3]             3\n",
      "         1110            [11]            11\n",
      "          492            [15]            15\n",
      "          263             [1]             1\n",
      "          223            [15]            15\n",
      "         1204            [12]            12\n",
      "          361             [8]             8\n",
      "          967             [9]             9\n",
      "          410             [5]             5\n",
      "          175            [13]            13\n",
      "         1317             [5]             5\n",
      "          503             [3]             3\n",
      "          945             [6]             6\n",
      "          576             [5]             5\n",
      "           91             [8]             8\n",
      "         1082            [10]            10\n",
      "         1408             [2]             2\n",
      "          788             [1]             1\n",
      "         1072            [11]            11\n",
      "          192             [1]             1\n",
      "         1364             [2]             2\n",
      "          722             [8]             8\n",
      "         1561             [6]             6\n",
      "          341             [6]             6\n",
      "          358            [12]            12\n",
      "         1306            [13]            13\n",
      "          529             [9]             9\n",
      "           64            [15]            15\n",
      "          640            [11]            11\n",
      "         1189            [15]            15\n",
      "          176            [14]            14\n",
      "         1100             [3]             3\n",
      "         1044             [3]             3\n",
      "         1381             [4]             4\n",
      "         1404            [15]            15\n",
      "           69             [7]             7\n",
      "          278            [13]            13\n",
      "          624             [6]             6\n",
      "          298             [8]             8\n",
      "          417             [4]             4\n",
      "          674             [6]             6\n",
      "         1077            [15]            15\n",
      "         1210            [14]            14\n",
      "          765            [15]            15\n",
      "          595            [11]            11\n",
      "          639             [2]             2\n",
      "          932             [4]             4\n",
      "         1410             [6]             6\n",
      "         1375             [7]             7\n",
      "         1000             [5]             5\n",
      "         1175            [12]            12\n",
      "           94             [2]             2\n",
      "         1218            [12]            12\n",
      "          395            [10]            10\n",
      "         1324            [15]            15\n",
      "          244             [2]             2\n",
      "          963             [3]             3\n",
      "          721            [11]            11\n",
      "          393             [1]             1\n",
      "          400             [0]             0\n",
      "         1329             [2]             2\n",
      "          782             [3]             3\n",
      "          146             [1]             1\n",
      "         1372            [12]            12\n",
      "         1236             [2]             2\n",
      "          927            [11]            11\n",
      "          524             [1]             1\n",
      "         1034             [2]             2\n",
      "         1519             [9]             9\n",
      "         1333             [5]             5\n",
      "         1373             [3]             3\n",
      "         1118             [3]             3\n",
      "          690             [6]             6\n",
      "          940             [4]             4\n",
      "          539             [3]             3\n",
      "          672             [0]             0\n",
      "         1252             [0]             0\n",
      "          548            [11]            11\n",
      "         1556            [10]            10\n",
      "         1004             [6]             6\n",
      "          986             [3]             3\n",
      "         1311             [5]             5\n",
      "          806             [2]             2\n",
      "         1502            [15]            15\n",
      "           30             [1]             1\n",
      "          845             [5]             5\n",
      "            8            [15]            15\n",
      "          519            [14]            14\n",
      "         1256            [14]            14\n",
      "         1234             [0]             0\n",
      "         1223             [1]             1\n",
      "         1291             [5]             5\n",
      "          742            [13]            13\n",
      "          861             [4]             4\n",
      "          934             [8]             8\n",
      "         1171             [0]             0\n",
      "           17            [11]            11\n",
      "         1382            [12]            12\n",
      "         1104             [8]             8\n",
      "          195             [7]             7\n",
      "         1582             [0]             0\n",
      "          271             [6]             6\n",
      "         1134             [5]             5\n",
      "          516             [8]             8\n",
      "          978            [11]            11\n",
      "          218             [1]             1\n",
      "          276            [14]            14\n",
      "          896             [8]             8\n",
      "          671             [0]             0\n",
      "          910             [6]             6\n",
      "         1397            [10]            10\n",
      "          789             [0]             0\n",
      "         1522             [0]             0\n",
      "          319             [0]             0\n",
      "           53            [13]            13\n",
      "          437             [8]             5\n",
      "         1421             [6]             6\n",
      "         1259             [5]             5\n",
      "           98            [13]            13\n",
      "         1253             [9]             9\n",
      "         1010            [11]            11\n",
      "          996             [3]             3\n",
      "          906            [15]            15\n",
      "          540             [3]             3\n",
      "         1157             [1]             1\n",
      "         1187             [1]             1\n",
      "          447             [2]             2\n",
      "         1331             [8]             8\n",
      "          125             [3]             3\n",
      "         1516            [10]            10\n",
      "         1108             [0]             0\n",
      "          506             [9]             9\n",
      "          983            [14]            14\n",
      "         1267            [13]            13\n",
      "          924             [8]             8\n",
      "          935            [15]            15\n",
      "          771             [4]             4\n",
      "          647             [6]             6\n",
      "         1507            [13]            13\n",
      "         1133             [3]             3\n",
      "          754             [7]             7\n",
      "         1489             [8]             8\n",
      "          597             [6]             6\n",
      "         1212            [11]            11\n",
      "          921            [11]            11\n",
      "          954             [0]             0\n",
      "          953             [0]             0\n",
      "         1087             [9]             9\n",
      "          837            [10]            10\n",
      "          705            [12]            12\n",
      "          995             [6]             6\n",
      "           12             [6]             6\n",
      "         1388             [1]             1\n",
      "         1475            [13]            13\n",
      "          807             [0]             0\n",
      "          602            [11]            11\n",
      "          502             [2]             2\n",
      "          719             [6]             6\n",
      "          811             [8]             8\n",
      "          245            [11]            11\n",
      "         1193            [14]            14\n",
      "          261            [15]            15\n",
      "          457             [7]             7\n",
      "          320             [2]             2\n",
      "\n",
      "Accuracy of 500 samples: 98.40%\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    filepath = r'C:\\Users\\SIDDHARTH SINGH\\Downloads\\Untitled Folder\\Merged_dataset.xlsx'\n",
    "    data_preprocessor = DataPreprocessor(filepath)\n",
    "    X_train, X_test, y_train, y_test = data_preprocessor.preprocess()\n",
    "\n",
    "    num_conditions = len(np.unique(y_train))\n",
    "    model_manager = MLPFaultModelManager(X_train.shape[1], num_conditions)\n",
    "    model_manager.train_models(X_train, y_train)\n",
    "\n",
    "    # Evaluate models before feedback\n",
    "    evaluator = ModelEvaluator(model_manager.models)\n",
    "    initial_evaluations = evaluator.evaluate_models(X_test, y_test)\n",
    "    print(\"Initial Model Evaluations:\")\n",
    "    for condition, eval in initial_evaluations.items():\n",
    "        print(f\"Condition {condition}: Accuracy = {eval['accuracy']:.2%}\")\n",
    "        print(eval['report'])\n",
    "\n",
    "    residual_calculator = ResidualCalculator(model_manager.models)\n",
    "    residuals = residual_calculator.calculate_residuals(X_test, y_test)\n",
    "\n",
    "    # Dynamic Bayesian Network feedback\n",
    "    dbn = DynamicBayesianNetwork(range(num_conditions))\n",
    "    feedback_evidence = {f'Condition_{i}': residuals[i].mean() > 0.5 for i in range(num_conditions)}\n",
    "    feedback_results = dbn.integrate_with_dbn(feedback_evidence)\n",
    "    state_prob = feedback_results.values\n",
    "    print(\"Feedback from DBN:\", feedback_results)\n",
    "\n",
    "    for condition in range(num_conditions):\n",
    "        if state_prob[1] > 0.5:  # Assuming the state '1' indicates a fault or need for re-training\n",
    "            print(f\"Re-training model for condition {condition} due to high probability of system fault.\")\n",
    "            y_condition = (y_train == condition).astype(int)\n",
    "            model_manager.models[condition].train(X_train, y_condition)\n",
    "\n",
    "    final_evaluations = evaluator.evaluate_models(X_test, y_test)\n",
    "    print(\"Final Model Evaluations after DBN Feedback:\")\n",
    "    for condition, eval in final_evaluations.items():\n",
    "        print(f\"Condition {condition}: Accuracy = {eval['accuracy']:.2%}\")\n",
    "        print(eval['report'])\n",
    "\n",
    "    # Evaluate on random samples\n",
    "    random_sample_results, accuracy = evaluator.predict_random_samples(X_test, y_test, 500)\n",
    "    print(\"\\nRandom Sample Evaluations (500 samples):\")\n",
    "    print(random_sample_results.to_string(index=False))\n",
    "    print(f\"\\nAccuracy of 500 samples: {accuracy:.2%}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3434d430",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
